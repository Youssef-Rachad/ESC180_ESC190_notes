---
title: Hashing
---
This lecture will cover the basics of how dictionaries work.
Hashing is the basic concept behind python dictionaries (also c++ maps but you don't need to know that!). Here we will see how hashing is different from simply storing elements in an array and the advantages we can draw in terms of element look up time complexity.


{{site.data.alerts.note}}
AÂ **map**Â is a collection ofÂ _(key, value)_Â pairs, where the keys are unique, but the values may not be. The two operations are:

- `get(key)`Â gets the value at the specific key
- `put(key, value)`Â puts the pair (key,value) in the map

_The term map also highlights the relationship between the keys and the values. You might have seen in MAT185 that mappings can be injective, surjective, bijective. In general, the map ADT is surjective (anything not in the values can be ignored) but not necessarily injective (hence why multiple keys can point to the same value)._
{{site.data.alerts.end}}

<u>For example, a python dictionary is an implementation of a map. How can we implement this?</u>

We can use a binary tree (specifically an AVL tree, don't need to know this). The pairs are stored as entries on this tree, sorted using a function that compares the keys. The operations $\verb #get#$Â andÂ $\verb#put#$Â takeÂ $\mathcal ğ‘‚(\logâ¡ ğ‘›)$Â time.

Use an array of values. We permit only integer keys and if the integers are big, the array may need to be large. If I want to store 100 pairs but one of my keys isÂ 10000, then my array will have a size of at leastÂ 10000! The advantage is takes constant time forÂ getÂ andÂ put.

Use hash tables. This is an extension of the array idea, but we don't need very large arrays, and keys are no longer limited to integers!

## Hash Tables

To implement a hash table, we can define a table (i.e. an array) of lengthÂ $\verb#TableSize#$. Then we can define a functionÂ `â„ğ‘ğ‘ â„(ğ‘˜ğ‘’ğ‘¦)`Â that maps keys to integer indices in the rangeÂ $0,\dots,\verb#TableSize#-1$. Assuming thatÂ `â„ğ‘ğ‘ â„(ğ‘˜ğ‘’ğ‘¦)`Â takes constant time, then we can implement theÂ  $\verb #get#$Â andÂ $\verb#put#$Â operations in constant time. Specifically, we ideally want this function to be anÂ **injection.**Â In other words, there is a one-to-one map between keys and indices. This is because if we have two keys that map to the same index, then we have a collision.  
  
However, in reality this is not possible. There is aÂ [countably](https://en.wikipedia.org/wiki/Countable_set)Â infinite number of keys, but only a finiteÂ Â $\verb#TableSize#$Â entries in the array. This isn't that bad though! For the most part, we can try to avoid collisions by distributing items evenly, and if there are collisions, we just need to find a way to resolve them.

## Hash Functions

From now on, we'll be working with integer keys. If we have a string as a key, we can simply convert it to its ASCII value. Remember from before that we want to save space, so we want to makeÂ Â $\verb#TableSize#$Â as small as possible, and for that reason, the hash functionÂ $â„ğ‘ğ‘ â„(ğ‘¥)=ğ‘¥$Â is bad. Instead, we can try $$â„ğ‘ğ‘ â„(ğ‘¥)=ğ‘¥\ \%\ \verb#TableSize#$$where the symbolÂ %Â is the modulo operator. This is a good hash function because it maps all integers to the rangeÂ $0,\dots,\verb#TableSize#-1$. However, it's not a good hash function because it's not an injection. For example, if we have a table of sizeÂ 10Â and we have the keysÂ $\{0,10,20,30,\dots,90\}$, then they will all map to the same index.  
  
Therefore, this hash function is only good if the keys we are working with are well distributed, in the sense that the hash function will map them to a relatively even distribution.  
  
If the key is a compound object (i.e. using a tuple as a key), we can construct the hash function$$â„ğ‘ğ‘ â„(ğ‘ ,ğ‘¥)=(hash_1(s)\times p_1+hash_2(x)\times p_2)\ \%\ \verb#TableSize#$$whereÂ $ğ‘_1$Â andÂ $ğ‘_2$Â are two prime numbers. The reason we wantÂ $ğ‘_1$Â andÂ $ğ‘_2$Â to be prime is so that we don't have empty space. If they shared a factor, i.e. the numberÂ 2, then it would be possible that we're skipping all the odd cells in the table.

## Collisions (Chaining)

No matter how good our hash function is, we'll always have collisions. How can we deal with them? One method is viaÂ **separate chaining**.Â The idea is that we can keep all the items that map to the same index in a linked list.  
  
The runtime depends on the number of elements in a list on average. We can define theÂ **load factor**$$\lambda=\frac{ğ‘}{\verb#TableSize#}$$where $N$ is the total number of entries. It represents the average number of items in a linked list. While the worst-case run time is stillÂ $\mathcal O(ğ‘›)$, because everything could collide with everything. However, if we have a good hash function and everything is distributed evenly, then the average run time isÂ $\mathcal O(\lambda)$. This is because if we're trying to lookup an item and it fails, we need to search all the nodes to ensure that, which isÂ $\lambda$Â operations. If it succeeds, then on average we need to performÂ $\lambda/2$Â operations. Either way, we getÂ $\mathcal O(\lambda)$. 
  
To ensure that we still have constant time operations, we wantÂ $\lambda\approx1$Â and we increase the table size whenever needed. However, we can't simplyÂ _only_Â double the size of the table, because the same hash function will no longer work for the keys stored previously. This is because the hash function depends on the table size! To resolve this, we'll double the table size and then rehash all the keys.

## Collisions (Probing)

Another way to deal with collisions is viaÂ **probing**. The idea is that we don't store the items in a linked list, but instead we store them in the table itself. If there is a collision, we can try to find another empty cell to store the item.  
  
For example, if we haveÂ **linear**Â probing, whenever we have a collision, we can try to store the item in the next cell. If that cell is occupied, then we try the next cell, and so on. This is easy to implement but in practice, it's not very good. The reason is that if we have a lot of collisions in close proximity to each other, it creates a cluster of occupied slots, making it more likely for subsequent collisions to occur in the same cluster, further exacerbating the problem.  
  
==There are more advanced methods of probing that can fix this issue.  ==
  
The loading factorÂ $\lambda$Â can be defined the same way, but note that with probing, we have $$\lambda\le1$$and the table is full ifÂ $\lambda=1$. Therefore, the interpretation is slightly different.